# 启动顺序（在 master 上操作，依赖 SSH 能免密登录）

先格式化 NameNode（只做一次，master 上）：

```bash
hdfs namenode -format
```

启动 ZooKeeper 集群（在每台或通过 ssh 远程）：

```bash
# 从 master 开启所有 zk
ssh user@master  "zkServer.sh start"
ssh user@worker1 "zkServer.sh start"
ssh user@worker2 "zkServer.sh start"

# 验证
ssh user@master  "zkServer.sh status"
ssh user@worker1 "zkServer.sh status"
ssh user@worker2 "zkServer.sh status"
```

启动 HDFS + YARN（从 master）：

```bash
# 这会通过 ssh 到 workers 启动 DataNode/NodeManager
start-dfs.sh
start-yarn.sh
```

检查每台机器 `jps`，应看到：

* master: NameNode, ResourceManager, SecondaryNameNode(if any), HMaster, QuorumPeerMain
* worker1/worker2: DataNode, NodeManager, HRegionServer, QuorumPeerMain

![image-20250912150331865](/home/sun/.config/Typora/typora-user-images/image-20250912150331865.png)

![image-20250912150414350](/home/sun/.config/Typora/typora-user-images/image-20250912150414350.png)

![image-20250912150355694](/home/sun/.config/Typora/typora-user-images/image-20250912150355694.png)

HDFS NameNode UI: `http://master:9870`![image-20250912150445172](/home/sun/.config/Typora/typora-user-images/image-20250912150445172.png)

![image-20250912150528824](/home/sun/.config/Typora/typora-user-images/image-20250912150528824.png)

四台机器：
![image-20250912201345126](/home/sun/.config/Typora/typora-user-images/image-20250912201345126.png)





YARN UI: `http://master:8088`

![image-20250912151058398](/home/sun/.config/Typora/typora-user-images/image-20250912151058398.png)

启动 HBase（master）：

```bash
start-hbase.sh
```

![image-20250912151025653](/home/sun/.config/Typora/typora-user-images/image-20250912151025653.png)

验证 HBase Master UI： `http://master:16010`![image-20250912151002258](/home/sun/.config/Typora/typora-user-images/image-20250912151002258.png)

# 分割文件

```python
import os
import math
import logging
# 用于处理老师发的文件
# 这个脚本用于将一个大的txt文件按每10000行分割成多个小文件
# 指定输入文件和输出目录
input_file_path = r"./data/sentences/sentences.txt"  # 输入的txt文件路径
output_directory = r'./data/sentences/files'  # 指定的输出目录路径
logging.basicConfig(level=logging.INFO)
logging.info('Starting file splitting')
# 打开原始txt文件,一共有9397023条句子
with open(input_file_path, 'r', encoding='utf-8') as input_file:
    lines = input_file.readlines()
# 计算总行数和文件数
total_lines = len(lines)
num_files = (total_lines + 9999) // 10000
# 分割文件
for i in range(num_files):
    logging.info(f'Processing file {i + 1}/{num_files}')
    start = i * 10000
    end = min((i + 1) * 10000, total_lines)
    output_filename = os.path.join(output_directory, f'file{i}.txt')

    # 写入分割后的内容到新文件
    with open(output_filename, 'w', encoding='utf-8') as output_file:
        output_file.writelines(lines[start:end])
```

![image-20250912160346685](/home/sun/.config/Typora/typora-user-images/image-20250912160346685.png)

# 上传文件

把分割好的文件存入hdfs

```python
import subprocess
import os
import glob
from math import ceil
try:
    from tqdm import tqdm
except Exception:
    tqdm = None
import logging
import sys

# 本地分割文件目录
LOCAL_DIR = "data/sentences/files"
# HDFS目标目录
HDFS_DIR = "/input/sentences/files"
# 检查的文件名
CHECK_FILE = "file0.txt"


def setup_logging(verbose: bool = False):
    logger = logging.getLogger("upload_hdfs")
    level = logging.DEBUG if verbose else logging.INFO
    logger.setLevel(level)
    handler = logging.StreamHandler(sys.stdout)
    fmt = logging.Formatter("%(asctime)s %(levelname)-7s: %(message)s", "%Y-%m-%d %H:%M:%S")
    handler.setFormatter(fmt)
    logger.handlers = [handler]
    return logger


def run_cmd(cmd, logger, check=True, max_output_lines=20):
    """Run command and capture output; return (returncode, stdout, stderr).

    To avoid noisy logs when uploading many files, cap the number of lines shown
    from stdout/stderr unless verbose mode is enabled.
    """
    logger.debug("运行命令: %s", " ".join(cmd))
    p = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
    out, err = p.communicate()
    if out:
        lines = out.strip().splitlines()
        if logger.level <= logging.DEBUG:
            logger.debug("stdout: %s", out.strip())
        else:
            snippet = "\n".join(lines[:max_output_lines])
            logger.info("stdout (showing first %d lines):\n%s", min(len(lines), max_output_lines), snippet)
    if err:
        lines = err.strip().splitlines()
        if logger.level <= logging.DEBUG:
            logger.debug("stderr: %s", err.strip())
        else:
            snippet = "\n".join(lines[:max_output_lines])
            logger.warning("stderr (showing first %d lines):\n%s", min(len(lines), max_output_lines), snippet)
    if check and p.returncode != 0:
        logger.error("命令失败，返回码 %d", p.returncode)
    return p.returncode, out, err


def upload_files(logger, verbose=False):
    pattern = os.path.join(LOCAL_DIR, "*")
    files = sorted(glob.glob(pattern))
    if not files:
        logger.error("本地目录没有找到要上传的文件 -> %s", pattern)
        logger.info("请确认相对路径是否正确或先运行数据生成脚本。")
        return False

    total = len(files)
    logger.info("开始上传 %d 个文件到 %s", total, HDFS_DIR)

    iterator = range(total)
    if tqdm is not None:
        iterator = tqdm(range(total), desc="上传进度", unit="file")

    for idx in iterator:
        f = files[idx]
        hdfs_target = f"{HDFS_DIR}/{os.path.basename(f)}"
        # 每个文件先检查是否存在（简短），若需要覆盖可在外部用 --overwrite 清空目录
        cmd_test = ["hdfs", "dfs", "-test", "-e", hdfs_target]
        rc_test, _, _ = run_cmd(cmd_test, logger, check=False)
        if rc_test == 0:
            # 文件已存在
            status = "exists"
            logger.debug("文件已存在：%s", hdfs_target)
        else:
            # 上传单个文件
            rc_put, out, err = run_cmd(["hdfs", "dfs", "-put", f, HDFS_DIR], logger, check=False)
            status = "ok" if rc_put == 0 else "fail"

        # 每 20 个文件输出一次当前状态（简短）
        if (idx + 1) % 20 == 0 or (idx + 1) == total:
            logger.info("已处理 %d/%d 文件. 最近文件: %s 状态=%s", idx + 1, total, os.path.basename(f), status)

        # 避免 tqdm 里重复输出太多 DEBUG 信息
        if not verbose:
            # 抑制每文件的 DEBUG 输出（run_cmd 已根据 logger level 控制详细输出）
            pass

    logger.info("上传过程完成（可能部分文件已存在或失败），目标目录：%s", HDFS_DIR)
    return True


def check_file(logger):
    hdfs_file = f"{HDFS_DIR}/{CHECK_FILE}"
    cmd_test = ["hdfs", "dfs", "-test", "-e", hdfs_file]
    rc, _, _ = run_cmd(cmd_test, logger, check=False)
    if rc == 0:
        logger.info("%s 存在，正在打印内容：", hdfs_file)
        cmd_cat = ["hdfs", "dfs", "-cat", hdfs_file]
        run_cmd(cmd_cat, logger, check=False)
    else:
        logger.warning("%s 不存在", hdfs_file)


if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description="Upload local split files to HDFS")
    parser.add_argument("--overwrite", action="store_true", help="overwrite existing files in HDFS")
    parser.add_argument("--verbose", action="store_true", help="show debug logs")
    args = parser.parse_args()

    logger = setup_logging(verbose=args.verbose)

    # if overwrite requested, create HDFS dir and remove existing contents
    if args.overwrite:
        logger.info("--overwrite enabled: will remove existing files under %s before upload", HDFS_DIR)
        run_cmd(["hdfs", "dfs", "-rm", "-r", "-f", HDFS_DIR], logger, check=False)
        run_cmd(["hdfs", "dfs", "-mkdir", "-p", HDFS_DIR], logger, check=False)

    ok = upload_files(logger)
    if ok:
        check_file(logger)


```







```bash
hadoop jar java_src/InvertedMapReduce/build/libs/InvertedMapReduce-1.0-SNAPSHOT.jar /input/sentences/files
```



![image-20250912200327137](/home/sun/.config/Typora/typora-user-images/image-20250912200327137.png)



![image-20250912200418774](/home/sun/.config/Typora/typora-user-images/image-20250912200418774.png)



![image-20250912201225483](/home/sun/.config/Typora/typora-user-images/image-20250912201225483.png)

```bash
hbase:004:0> scan 'InvertedIndexTable', {LIMIT => 20}
ROW                                      COLUMN+CELL                                                                                                           
 0                                       column=info:index, timestamp=2025-09-12T20:01:50.044, value=file10.txt:20;                                            
 00                                      column=info:index, timestamp=2025-09-12T20:01:50.044, value=file115.txt:8;                                            
 000                                     column=info:index, timestamp=2025-09-12T20:01:50.044, value=file112.txt:73;                                           
 0000                                    column=info:index, timestamp=2025-09-12T20:01:50.044, value=file103.txt:1;                                            
 00003591                                column=info:index, timestamp=2025-09-12T20:01:50.044, value=file113.txt:1;                                            
 0001                                    column=info:index, timestamp=2025-09-12T20:01:50.044, value=file101.txt:1;                                            
 0002                                    column=info:index, timestamp=2025-09-12T20:01:50.044, value=file112.txt:1;                                            
 000ft                                   column=info:index, timestamp=2025-09-12T20:01:50.044, value=file111.txt:1;                                            
 000kilowatt                             column=info:index, timestamp=2025-09-12T20:01:50.044, value=file111.txt:1;                                            
 000kw                                   column=info:index, timestamp=2025-09-12T20:01:50.044, value=file100.txt:1;                                            
 000lb                                   column=info:index, timestamp=2025-09-12T20:01:50.044, value=file110.txt:1;                                            
 000mw                                   column=info:index, timestamp=2025-09-12T20:01:50.044, value=file115.txt:1;                                            
 000s                                    column=info:index, timestamp=2025-09-12T20:01:50.044, value=file105.txt:1;                                            
 000sqm                                  column=info:index, timestamp=2025-09-12T20:01:50.044, value=file112.txt:1;                                            
 000th                                   column=info:index, timestamp=2025-09-12T20:01:50.044, value=file11.txt:1;file115.txt:1;                               
 001                                     column=info:index, timestamp=2025-09-12T20:01:50.044, value=file109.txt:1;file108.txt:1;                              
 002                                     column=info:index, timestamp=2025-09-12T20:01:50.044, value=file110.txt:3;                                            
 002nd                                   column=info:index, timestamp=2025-09-12T20:01:50.044, value=file111.txt:1;                                            
 004                                     column=info:index, timestamp=2025-09-12T20:01:50.044, value=file101.txt:1;                                            
 005                                     column=info:index, timestamp=2025-09-12T20:01:50.044, value=file103.txt:1;                                            
20 row(s)
Took 0.4353 seconds     
```







