很好 — 我把 **从伪分布到最小三节点完全分布式集群** 的所有改动、该在哪台机器上执行哪些命令、哪些文件要改、哪些文件要复制到哪个路径，都按可操作的步骤写清楚， **你按着执行就能把本地伪分布迁移到三节点完全分布** 。我用 `master` / `worker1` / `worker2` 作为主机名示例；把它替换成你实际的主机名或 IP 即可。

# 总体准备

在每台机器上都要做的事（先做这些，再做下一步的分发）：

* 确保系统时间同步（推荐安装 chrony/ntp）。
* 安装相同版本的 JDK（1.8），并把 `JAVA_HOME` 设置到相同路径（例如 `/usr/local/java/jdk1.8.0_461`）。
* 把 Hadoop/HBase/ZooKeeper 的安装包解压到相同路径（例如 `/usr/local/hadoop`、`/usr/local/hbase`、`/usr/local/zookeeper`）。（下面有一次性复制命令）
* 创建用于运行 Hadoop/HBase 的同名用户（可选但推荐统一，例如 `sun`），并在三台上使用同一用户进行操作。
* 打开并启动 SSH 服务（每台都要）：
  ```bash
  sudo apt update
  sudo apt install -y openssh-server
  sudo systemctl enable ssh
  sudo systemctl start ssh
  ```

# /etc/hosts（在每台机器上都要相同）

把三台的 IP 与主机名都写入每台的 `/etc/hosts`，例如（在 master、worker1、worker2 三台都做相同修改）：

```
192.168.43.76 master
192.168.43.3 worker1
192.168.43.57 worker2
```

保存后测试：

```bash
ping -c 1 master
ping -c 1 worker1
ping -c 1 worker2
```

# SSH 无密码登录（在 master 上执行）

在 master 上生成密钥并分发到所有节点（包括自己）：

这里的user写的是目标机器的用户名

```bash
# 在 master 上
ssh-keygen -t rsa -N "" -f ~/.ssh/id_rsa
ssh-copy-id -i ~/.ssh/id_rsa.pub user@master
ssh-copy-id -i ~/.ssh/id_rsa.pub user@worker1
ssh-copy-id -i ~/.ssh/id_rsa.pub user@worker2
# 测试
ssh kxg@worker1
ssh xwb@worker2
```

`user` 是你实际的登录用户名（如 `sun`）。如果能免密登录，则 OK。

在每台机器用户的 `~/.bashrc` 中加入相同的环境变量（示例）：

```bash
export JAVA_HOME=/usr/local/java/jdk1.8.0_461
export HADOOP_HOME=/usr/local/hadoop
export HBASE_HOME=/usr/local/hbase
export ZOOKEEPER_HOME=/usr/local/zookeeper
export PATH=$JAVA_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$HBASE_HOME/bin:$ZOOKEEPER_HOME/bin:$PATH
```

然后 `source ~/.bashrc`。

# Hadoop 配置改动 (从伪分布修改为完全分布)

在 master（或本地配置机）编辑并准备好这些配置文件，然后分发到 worker 上（用 scp）。

关键文件位置：`$HADOOP_HOME/etc/hadoop/`

要修改的文件及内容（把 `localhost` 换成 `master` 或实际主机名）：

core-site.xml

```xml
<configuration>
  <property>
    <name>fs.defaultFS</name>
    <value>hdfs://master:9000</value>
  </property>
  <property>
    <name>hadoop.tmp.dir</name>
    <value>/usr/local/hadoop/tmp</value>
  </property>
</configuration>
```

hdfs-site.xml

```xml
<configuration>
  <property>
    <name>dfs.replication</name>
    <value>3</value> <!-- 三个节点，副本数设为3 -->
  </property>
  <property>
    <name>dfs.namenode.name.dir</name>
    <value>file:///usr/local/hadoop/hdfs/namenode</value>
  </property>
  <property>
    <name>dfs.datanode.data.dir</name>
    <value>file:///usr/local/hadoop/hdfs/datanode</value>
  </property>
</configuration>
```

在每台机器上 `dfs.namenode.name.dir` / `dfs.datanode.data.dir` 路径可以相同（file:/// 表示本地路径），但 NameNode 的数据目录只在 master 格式化一次并保留。

yarn-site.xml

```xml
<?xml version="1.0"?>
<!--
  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License. See accompanying LICENSE file.
-->
<configuration>
  <property>
    <name>yarn.nodemanager.aux-services</name>
    <value>mapreduce_shuffle</value>
  </property>
  <property>
    <name>yarn.resourcemanager.hostname</name>
    <value>master</value>
  </property>
  <property>
		<name>yarn.application.classpath</name>
		<value>
		  $HADOOP_HOME/etc/hadoop,
		  $HADOOP_HOME/share/hadoop/common/*,
		  $HADOOP_HOME/share/hadoop/common/lib/*,
		  $HADOOP_HOME/share/hadoop/hdfs/*,
		  $HADOOP_HOME/share/hadoop/hdfs/lib/*,
		  $HADOOP_HOME/share/hadoop/mapreduce/*,
		  $HADOOP_HOME/share/hadoop/mapreduce/lib/*,
		  $HADOOP_HOME/share/hadoop/yarn/*,
		  $HADOOP_HOME/share/hadoop/yarn/lib/*
		</value>
	</property>
	<property>
		<name>yarn.log-aggregation-enable</name>
		<value>true</value>
	</property>
	<property>
		<name>yarn.nodemanager.remote-app-log-dir</name>
		<value>/tmp/logs</value>  <!-- 或 HDFS 上的 path: hdfs://localhost:9000/tmp/logs -->
	</property>
</configuration>

```

mapred-site.xml（从模板复制）

```xml
<configuration>
  <property>
    <name>mapreduce.framework.name</name>
    <value>yarn</value>
  </property>
</configuration>
```

workers（或 slaves，取决于 hadoop 目录）

在 `$HADOOP_HOME/etc/hadoop/workers` 写上 worker 主机名（每行一个）：

```
worker1
worker2
```

这样 `start-dfs.sh` / `start-yarn.sh` 会在这些主机上启动 DataNode / NodeManager。

masters（可选，用于 SecondaryNameNode）

在 `$HADOOP_HOME/etc/hadoop/masters` 写 SecondaryNameNode 要跑的主机（可以放 worker2 或专门节点）：

```
worker2
```

# ZooKeeper 配置（每台都要改并创建 myid）

在 `$ZOOKEEPER_HOME/conf/zoo.cfg`（在三台机器上放相同文件）：

```properties

# tickTime CS通信心跳数
# Zookeeper 服务器之间或客户端与服务器之间维持心跳的时间间隔，也就是每个 tickTime 时间就会发送一个心跳。tickTime以毫秒为单位。
tickTime=2000

# initLimit LF初始通信时限
# 集群中的follower服务器(F)与leader服务器(L)之间初始连接时能容忍的最多心跳数（tickTime的数量）。
initLimit=10

# syncLimit LF同步通信时限
# 集群中的follower服务器与leader服务器之间请求和应答之间能容忍的最多心跳数（tickTime的数量）。
syncLimit=5


# dataDir：数据文件目录
# Zookeeper保存数据的目录，默认情况下，Zookeeper将写数据的日志文件也保存在这个目录里。
dataDir=/usr/local/zookeeper/data

# dataLogDir：日志文件目录，Zookeeper保存日志文件的目录。
dataLogDir=/usr/local/zookeeper/logs


# the port at which the clients will connect
# clientPort：客户端连接端口，通过这个端口可以连接zookeeper服务
clientPort=2181

# the maximum number of client connections.
# increase this if you need to handle more clients
#maxClientCnxns=60


# Be sure to read the maintenance section of the 
# administrator guide before turning on autopurge.
# http://zookeeper.apache.org/doc/current/zookeeperAdmin.html#sc_maintenance
# The number of snapshots to retain in dataDir
#autopurge.snapRetainCount=3
# Purge task interval in hours
# Set to "0" to disable auto purge feature
#autopurge.purgeInterval=1

## Metrics Providers
# https://prometheus.io Metrics Exporter
#metricsProvider.className=org.apache.zookeeper.metrics.prometheus.PrometheusMetricsProvider
#metricsProvider.httpPort=7000
#metricsProvider.exportJvmInfo=true

#2888 选举端口
#3888 心跳端口
# 服务器名称与地址：集群信息（服务器编号，服务器地址，选举端口，心跳端口），服务器地址可以用ip,可以用域名，用域名就需要在/etc/hosts文件进行指定了
# 这个配置项的书写格式比较特殊，规则如下：
server.1=master:2888:3888
server.2=worker1:2888:3888
server.3=worker2:2888:3888
```

在每台机器创建 dataDir 并写 myid（数字对应上面 server.N）：**(这个不用管，已经改完了)**

```bash
# 在 master
mkdir -p /usr/local/zookeeper/data /usr/local/zookeeper/logs
echo "1" > /usr/local/zookeeper/data/myid

# 在 worker1
ssh kxg@worker1 "mkdir -p /usr/local/zookeeper/data /usr/local/zookeeper/logs; echo '2' > /usr/local/zookeeper/data/myid"

# 在 worker2
ssh xwb@worker2 "mkdir -p /usr/local/zookeeper/data /usr/local/zookeeper/logs; echo '3' > /usr/local/zookeeper/data/myid"
```


# HBase 配置改动（修改 hbase-site.xml，并分发）

在 `$HBASE_HOME/conf/hbase-site.xml`（把 `localhost` 改成集群主机名并分发）：

```xml
<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<!--
/*
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
--> 
 <!--
    The following properties are set for running HBase as a single process on a
    developer workstation. With this configuration, HBase is running in
    "stand-alone" mode and without a distributed file system. In this mode, and
    without further configuration, HBase and ZooKeeper data are stored on the
    local filesystem, in a path under the value configured for `hbase.tmp.dir`.
    This value is overridden from its default value of `/tmp` because many
    systems clean `/tmp` on a regular basis. Instead, it points to a path within
    this HBase installation directory.

    Running against the `LocalFileSystem`, as opposed to a distributed
    filesystem, runs the risk of data integrity issues and data loss. Normally
    HBase will refuse to run in such an environment. Setting
    `hbase.unsafe.stream.capability.enforce` to `false` overrides this behavior,
    permitting operation. This configuration is for the developer workstation
    only and __should not be used in production!__

    See also https://hbase.apache.org/book.html#standalone_dist
  -->
<configuration>
  <!-- 使用 HDFS 存储 HBase 数据 -->
  <property>
    <name>hbase.rootdir</name>
    <value>hdfs://master:9000/hbase</value>
  </property>

  <!-- 表示这是一个分布式集群（即便是伪分布式） -->
  <property>
    <name>hbase.cluster.distributed</name>
    <value>true</value>
  </property>

  <!-- ZooKeeper 配置 -->
  <property>
    <name>hbase.zookeeper.quorum</name>
    <value>master,worker1,worker2</value>
  </property>
  
  <property>
    <name>hbase.zookeeper.property.clientPort</name>
    <value>2181</value>
  </property>
</configuration>
```

hbase-env.sh 设置 Java 与不由 HBase 管理 ZooKeeper：

```bash
export JAVA_HOME=/usr/local/java/jdk1.8.0_461
export HBASE_MANAGES_ZK=false
```

在 `$HBASE_HOME/conf/regionservers` 写上 regionserver 主机名（每行一个）：

```
worker1
worker2
```

把 `$HBASE_HOME/conf` 分发到 worker：

```bash
scp -r $HBASE_HOME/conf user@worker1:$HBASE_HOME/
scp -r $HBASE_HOME/conf user@worker2:$HBASE_HOME/
```

# 启动顺序（在 master 上操作，依赖 SSH 能免密登录）

先格式化 NameNode（只做一次，master 上）：

```bash
hdfs namenode -format
```

启动 ZooKeeper 集群（在每台或通过 ssh 远程）：

```bash
# 从 master 开启所有 zk
ssh user@master  "$ZOOKEEPER_HOME/bin/zkServer.sh start"
ssh user@worker1 "$ZOOKEEPER_HOME/bin/zkServer.sh start"
ssh user@worker2 "$ZOOKEEPER_HOME/bin/zkServer.sh start"

# 验证
ssh user@master  "$ZOOKEEPER_HOME/bin/zkServer.sh status"
ssh user@worker1 "$ZOOKEEPER_HOME/bin/zkServer.sh status"
ssh user@worker2 "$ZOOKEEPER_HOME/bin/zkServer.sh status"
```

启动 HDFS + YARN（从 master）：

```bash
# 这会通过 ssh 到 workers 启动 DataNode/NodeManager
start-dfs.sh
start-yarn.sh
```

检查每台机器 `jps`，应看到：

* master: NameNode, ResourceManager, SecondaryNameNode(if any), HMaster, QuorumPeerMain
* worker1/worker2: DataNode, NodeManager, HRegionServer, QuorumPeerMain

启动 HBase（master）：

```bash
start-hbase.sh
```

验证 HBase Master UI： `http://master:16010`，HDFS NameNode UI: `http://master:9870`，YARN UI: `http://master:8088`




# 常用分发命令（把配置从 master 推到所有节点）

把配置目录统一分发（示例）：

```bash
# hadoop
scp -r $HADOOP_HOME/etc/hadoop user@worker1:$HADOOP_HOME/etc/
scp -r $HADOOP_HOME/etc/hadoop user@worker2:$HADOOP_HOME/etc/

# hbase
scp -r $HBASE_HOME/conf user@worker1:$HBASE_HOME/
scp -r $HBASE_HOME/conf user@worker2:$HBASE_HOME/

# zookeeper
scp $ZOOKEEPER_HOME/conf/zoo.cfg user@worker1:$ZOOKEEPER_HOME/conf/
scp $ZOOKEEPER_HOME/conf/zoo.cfg user@worker2:$ZOOKEEPER_HOME/conf/
```

# 端口与防火墙提醒

确保节点间以下端口能互通（例）：

* SSH 22
* HDFS NameNode 9000; NameNode UI 9870; DataNode 9864 等
* YARN ResourceManager 8032/8088；NodeManager端口
* ZooKeeper 2181 (client), 2888/3888 (quorum)
* HBase Master 16000/16010；HRegionServer 16201 等

  可用 `ufw` 开放端口或临时禁用防火墙（仅在受信网络）：

```bash
sudo ufw allow 22
sudo ufw allow 9000
sudo ufw allow 9870
# 或 sudo ufw disable
```

# 验证与排错顺序

* 在每台机器运行 `jps`，核验对应进程是否启动。
* 在 master 上运行 `hdfs dfsadmin -report` 查看 DataNode 报告。
* 查看 HBase、HDFS、ZooKeeper 日志（`$HBASE_HOME/logs`、`$HADOOP_HOME/logs`、`$ZOOKEEPER_HOME/logs`）。
* 如果 `start-dfs.sh` 无法 ssh 到某台，请检查 `ssh user@thathost` 是否免密且 `~/.bashrc` 无阻塞输出（SSH non-interactive 要避免 echo 等）。

# 从你当前伪分布配置要改动的“具体地方”

你当前伪分布里主要是 `localhost`、`dfs.replication=1`、`hbase.zookeeper.quorum=localhost` 等。迁移到三节点完全分布需做的替换：

* `core-site.xml` `fs.defaultFS`：`hdfs://localhost:9000` → `hdfs://master:9000`
* `hdfs-site.xml` `dfs.replication`：`1` → `3`；并为每台 DataNode 设置 `dfs.datanode.data.dir`（本地路径）
* `yarn-site.xml` `yarn.resourcemanager.hostname`：`localhost` → `master`
* `workers`（原 `slaves`）：从只含 `localhost` 改为写入 `worker1`, `worker2`
* `hbase-site.xml` `hbase.rootdir`：`hdfs://localhost:9000/hbase` → `hdfs://master:9000/hbase`
* `hbase-site.xml` `hbase.zookeeper.quorum`：`localhost` → `master,worker1,worker2`
* ZooKeeper 的 `zoo.cfg`：从单机 config 改为包含 `server.1=...` / `server.2=...` / `server.3=...`，并在每台写入对应 `myid`。
* `hadoop-env.sh` / `hbase-env.sh`：确保 `JAVA_HOME` 在每台一致。

# 小结与推荐顺序（执行步骤清单）

* 在三台上准备 JDK、解压软件、设置环境变量、启动 ssh、更新 /etc/hosts。
* 在 master 上生成 SSH key 并 `ssh-copy-id` 到 worker1、worker2。
* 在 master 上编辑好所有配置文件（Hadoop、ZooKeeper、HBase）。
* 分发配置并在所有节点上创建必要目录并设置权限。
* 在三台上创建 ZooKeeper data 目录并写 `myid`。
* 在 master 上 `hdfs namenode -format`（只做一次）。
* 启动 ZooKeeper 三节点。
* 在 master 上 `start-dfs.sh`、`start-yarn.sh`。
* 启动 HBase（`start-hbase.sh`）。
* 验证集群状态并修正日志中报错。
