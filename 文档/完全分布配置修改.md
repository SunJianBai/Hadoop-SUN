# 总体准备

在每台机器上都要做的事（先做这些，再做下一步的分发）：

* 确保系统时间同步（推荐安装 chrony/ntp）。
* 安装相同版本的 JDK（1.8），并把 `JAVA_HOME` 设置到相同路径（例如 `/usr/local/java/jdk1.8.0_461`）。
* 把 Hadoop/HBase/ZooKeeper 的安装包解压到相同路径（例如 `/usr/local/hadoop`、`/usr/local/hbase`、`/usr/local/zookeeper`）。（下面有一次性复制命令）
* 创建用于运行 Hadoop/HBase 的同名用户（可选但推荐统一，例如 `sun`），并在三台上使用同一用户进行操作。
* 打开并启动 SSH 服务（每台都要）：
  ```bash
  sudo apt update
  sudo apt install -y openssh-server
  sudo systemctl enable ssh
  sudo systemctl start ssh
  ```

把三台的 IP 与主机名都写入每台的 `/etc/hosts`，例如（在 master、worker1、worker2 三台都做相同修改）

```
192.168.43.76 master
192.168.43.3 worker1
192.168.43.57 worker2
```

保存后测试：

```bash
ping -c 1 master
ping -c 1 worker1
ping -c 1 worker2
```

# SSH 无密码登录（在 master 上执行）

在 master 上生成密钥并分发到所有节点（包括自己）：

这里的user写的是目标机器的用户名

```bash
# 在 master 上
ssh-keygen -t rsa -N "" -f ~/.ssh/id_rsa
ssh-copy-id -i ~/.ssh/id_rsa.pub user@master
ssh-copy-id -i ~/.ssh/id_rsa.pub user@worker1
ssh-copy-id -i ~/.ssh/id_rsa.pub user@worker2
# 测试
ssh kxg@worker1
ssh xwb@worker2
```

`user` 是你实际的登录用户名（如 `sun`）。如果能免密登录，则 OK。

在每台机器用户的 `~/.bashrc` 中加入相同的环境变量（示例）：

```bash
export JAVA_HOME=/usr/local/java/jdk1.8.0_461
export HADOOP_HOME=/usr/local/hadoop
export HBASE_HOME=/usr/local/hbase
export ZOOKEEPER_HOME=/usr/local/zookeeper
export PATH=$JAVA_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$HBASE_HOME/bin:$ZOOKEEPER_HOME/bin:$PATH
```

然后 `source ~/.bashrc`。

# Hadoop 配置改动 (从伪分布修改为完全分布)

在 master（或本地配置机）编辑并准备好这些配置文件，然后分发到 worker 上（用 scp）。

关键文件位置：`$HADOOP_HOME/etc/hadoop/`

要修改的文件及内容（把 `localhost` 换成 `master` 或实际主机名）：

core-site.xml

```xml
<configuration>
  <property>
    <name>fs.defaultFS</name>
    <value>hdfs://master:9000</value>
  </property>
  <property>
    <name>hadoop.tmp.dir</name>
    <value>/usr/local/hadoop/tmp</value>
  </property>
</configuration>
```

hdfs-site.xml

```xml
<configuration>
  <property>
    <name>dfs.replication</name>
    <value>3</value> <!-- 三个节点，副本数设为3 -->
  </property>
  <property>
    <name>dfs.namenode.name.dir</name>
    <value>file:///usr/local/hadoop/hdfs/namenode</value>
  </property>
  <property>
    <name>dfs.datanode.data.dir</name>
    <value>file:///usr/local/hadoop/hdfs/datanode</value>
  </property>
</configuration>
```

在每台机器上 `dfs.namenode.name.dir` / `dfs.datanode.data.dir` 路径可以相同（file:/// 表示本地路径），但 NameNode 的数据目录只在 master 格式化一次并保留。

yarn-site.xml

```xml
<?xml version="1.0"?>
<!--
  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License. See accompanying LICENSE file.
-->
<configuration>
  <property>
    <name>yarn.nodemanager.aux-services</name>
    <value>mapreduce_shuffle</value>
  </property>
  <property>
    <name>yarn.resourcemanager.hostname</name>
    <value>master</value>
  </property>
  <property>
		<name>yarn.application.classpath</name>
		<value>
		  $HADOOP_HOME/etc/hadoop,
		  $HADOOP_HOME/share/hadoop/common/*,
		  $HADOOP_HOME/share/hadoop/common/lib/*,
		  $HADOOP_HOME/share/hadoop/hdfs/*,
		  $HADOOP_HOME/share/hadoop/hdfs/lib/*,
		  $HADOOP_HOME/share/hadoop/mapreduce/*,
		  $HADOOP_HOME/share/hadoop/mapreduce/lib/*,
		  $HADOOP_HOME/share/hadoop/yarn/*,
		  $HADOOP_HOME/share/hadoop/yarn/lib/*
		</value>
	</property>
	<property>
		<name>yarn.log-aggregation-enable</name>
		<value>true</value>
	</property>
	<property>
		<name>yarn.nodemanager.remote-app-log-dir</name>
		<value>/tmp/logs</value>  <!-- 或 HDFS 上的 path: hdfs://localhost:9000/tmp/logs -->
	</property>
</configuration>

```

mapred-site.xml（从模板复制）

```xml
<configuration>
  <property>
    <name>mapreduce.framework.name</name>
    <value>yarn</value>
  </property>
</configuration>
```

workers（或 slaves，取决于 hadoop 目录）

在 `$HADOOP_HOME/etc/hadoop/workers` 写上 worker 主机名（每行一个）：

```
worker1
worker2
```

这样 `start-dfs.sh` / `start-yarn.sh` 会在这些主机上启动 DataNode / NodeManager。

masters（可选，用于 SecondaryNameNode）

在 `$HADOOP_HOME/etc/hadoop/masters` 写 SecondaryNameNode 要跑的主机（可以放 worker2 或专门节点）：

```
worker2
```

# ZooKeeper 配置（每台都要改并创建 myid）

在 `$ZOOKEEPER_HOME/conf/zoo.cfg`（在三台机器上放相同文件）：

```properties
tickTime=2000
dataDir=/usr/local/zookeeper/data
dataLogDir=/usr/local/zookeeper/logs
clientPort=2181
initLimit=10
syncLimit=5
server.1=master:2888:3888
server.2=worker1:2888:3888
server.3=worker2:2888:3888
```

在每台机器创建 dataDir 并写 myid（数字对应上面 server.N）：**(这个不用管，已经改完了)**

```bash
# 在 master
mkdir -p /usr/local/zookeeper/data /usr/local/zookeeper/logs
echo "1" > /usr/local/zookeeper/data/myid

# 在 worker1
ssh kxg@worker1 "mkdir -p /usr/local/zookeeper/data /usr/local/zookeeper/logs; echo '2' > /usr/local/zookeeper/data/myid"

# 在 worker2
ssh xwb@worker2 "mkdir -p /usr/local/zookeeper/data /usr/local/zookeeper/logs; echo '3' > /usr/local/zookeeper/data/myid"
```

# HBase 配置改动（修改 hbase-site.xml，并分发）

在 `$HBASE_HOME/conf/hbase-site.xml`（把 `localhost` 改成集群主机名并分发）：

```xml
<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<!--
/*
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
--> 
 <!--
    The following properties are set for running HBase as a single process on a
    developer workstation. With this configuration, HBase is running in
    "stand-alone" mode and without a distributed file system. In this mode, and
    without further configuration, HBase and ZooKeeper data are stored on the
    local filesystem, in a path under the value configured for `hbase.tmp.dir`.
    This value is overridden from its default value of `/tmp` because many
    systems clean `/tmp` on a regular basis. Instead, it points to a path within
    this HBase installation directory.

    Running against the `LocalFileSystem`, as opposed to a distributed
    filesystem, runs the risk of data integrity issues and data loss. Normally
    HBase will refuse to run in such an environment. Setting
    `hbase.unsafe.stream.capability.enforce` to `false` overrides this behavior,
    permitting operation. This configuration is for the developer workstation
    only and __should not be used in production!__

    See also https://hbase.apache.org/book.html#standalone_dist
  -->
<configuration>
  <!-- 使用 HDFS 存储 HBase 数据 -->
  <property>
    <name>hbase.rootdir</name>
    <value>hdfs://master:9000/hbase</value>
  </property>

  <!-- 表示这是一个分布式集群（即便是伪分布式） -->
  <property>
    <name>hbase.cluster.distributed</name>
    <value>true</value>
  </property>

  <!-- ZooKeeper 配置 -->
  <property>
    <name>hbase.zookeeper.quorum</name>
    <value>master,worker1,worker2</value>
  </property>
  
  <property>
    <name>hbase.zookeeper.property.clientPort</name>
    <value>2181</value>
  </property>
</configuration>
```

hbase-env.sh 设置 Java 与不由 HBase 管理 ZooKeeper：

```bash
export JAVA_HOME=/usr/local/java/jdk1.8.0_461
export HBASE_MANAGES_ZK=false
```

在 `$HBASE_HOME/conf/regionservers` 写上 regionserver 主机名（每行一个，不需要写master）：

```
worker1
worker2
```
